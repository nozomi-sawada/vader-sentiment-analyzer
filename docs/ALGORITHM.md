# VADER Algorithm Details

**English** | [æ—¥æœ¬èª](#æ—¥æœ¬èªç‰ˆ)

This document explains the implementation details and mathematical definitions of the VADER algorithm.

---

## Table of Contents

1. [Algorithm Overview](#1-algorithm-overview)
2. [Analysis Unit: Sentence-Level vs Document-Level](#2-analysis-unit-sentence-level-vs-document-level)
3. [Lexicon-Based Scoring](#3-lexicon-based-scoring)
4. [Grammatical Adjustments](#4-grammatical-adjustments)
5. [Score Normalization](#5-score-normalization)
6. [Implementation Details](#6-implementation-details)

---

## 1. Algorithm Overview

VADER calculates sentiment scores through the following steps:

```
Input Text
    â†“
[0] Sentence Splitting (Optional)
    â†“
[1] Tokenization & Lexicon Score Retrieval
    â†“
[2] Grammatical Score Adjustments
    â†“
[3] Normalization & Compound Score Calculation
    â†“
Output: {compound, pos, neg, neu}
```

---

## 2. Analysis Unit: Sentence-Level vs Document-Level

### 2.1 VADER Paper Recommendation

The VADER paper (Hutto & Gilbert, 2014) **recommends sentence-level analysis**:

> "VADER performs **sentence-level sentiment analysis**"
> 
> "decomposing paragraphs, articles/reports/publications, or novels into **sentence-level analyses**"

### 2.2 Why Sentence-Level?

#### Reason 1: Context-Dependent Sentiment

Example where sentiment changes by sentence:

```
Document: "I love the design. But the quality is terrible."

Document-level analysis:
  â†’ Compound: +0.12 (overall slightly positive?)

Sentence-level analysis:
  S1: "I love the design." â†’ +0.6369 (clearly positive)
  S2: "But the quality is terrible." â†’ -0.5267 (clearly negative)
```

#### Reason 2: Paper Evaluation Data

All VADER paper evaluations use sentence-level units:

| Dataset | Sample Count | Unit |
|---------|-------------|------|
| Movie reviews | 10,605 | sentence-level snippets |
| Product reviews | 3,708 | sentence-level snippets |
| News articles | 5,190 | sentence-level snippets |

#### Reason 3: Grammar Rules Scope

VADER's grammar rules (negation, "but", etc.) function within sentences:

```
"The product is good. But I don't like it."

Sentence-level:
  S1: "good" â†’ positive (no "but" effect)
  S2: "like" â†’ negated (affected by "don't")

Document-level:
  "good" â†’ attenuated by "but" (applied across sentences)
  "like" â†’ negated
  â†’ Grammar rules applied in unintended ways
```

### 2.3 Sentence Splitting Algorithm

#### Basic Splitting Pattern

```javascript
function splitIntoSentences(text) {
    // Split by periods, exclamation marks, question marks
    return text.split(/([.!?]+\s+)/);
}
```

#### Abbreviation Protection Implementation

Prevents erroneous splits at common abbreviations:

```javascript
function splitIntoSentences(text) {
    // Step 1: Protect abbreviation periods
    const protectedText = text
        .replace(/\b(Mr|Mrs|Ms|Dr|Prof|Sr|Jr)\./gi, '$1<PERIOD>')
        .replace(/\b([A-Z])\./g, '$1<PERIOD>'); // Initials
    
    // Step 2: Split sentences
    const sentences = protectedText
        .split(/([.!?]+\s+)/)
        .reduce((acc, part, i, arr) => {
            if (i % 2 === 0) {
                const sentence = (part + (arr[i + 1] || '')).trim();
                if (sentence.length > 0) {
                    acc.push(sentence.replace(/<PERIOD>/g, '.'));
                }
            }
            return acc;
        }, []);
    
    return sentences;
}
```

**Protected Abbreviations List:**

| Category | Examples |
|----------|----------|
| Titles | Mr., Mrs., Ms., Dr., Prof. |
| Suffixes | Sr., Jr. |
| Others | vs., etc., Inc., Ltd., Corp., Co. |
| Initials | M., T., J., H., etc. |

#### Sentence Splitting Example

```
Input: 
"Mr. M. T. John and J. H. Samuel presented their papers."

Incorrect split (without protection):
  S1: "Mr."
  S2: "M."
  S3: "T."
  ...

Correct split (with protection):
  S1: "Mr. M. T. John and J. H. Samuel presented their papers."
```

### 2.4 Aggregation Methods

For multiple sentences in sentence-level analysis:

#### Method 1: Mean

```javascript
const avgCompound = sentences.reduce((sum, s) => sum + s.compound, 0) / sentences.length;
```

**Use case**: When you want overall sentiment tendency

#### Method 2: Category Count

```javascript
const posCount = sentences.filter(s => s.compound >= 0.05).length;
const negCount = sentences.filter(s => s.compound <= -0.05).length;
const neuCount = sentences.length - posCount - negCount;
```

**Use case**: When analyzing sentiment composition

#### Method 3: Weighted Average

```javascript
// Weight by sentence length
const weighted = sentences.reduce((sum, s) => {
    const weight = s.sentiments.length;
    return sum + (s.compound * weight);
}, 0) / totalTokens;
```

**Use case**: When emphasizing longer sentences

---

## 3. Lexicon-Based Scoring

### 3.1 Lexicon Structure

Each word has the following information:

```
Word    Mean Score  Std Dev    Human Ratings
good    1.9         0.94       [2,2,2,1,2,...]
bad     -1.5        0.75       [-2,-1,-2,-2,...]
```

**Score Range**: -4 (extremely negative) to +4 (extremely positive)

**Standard Deviation Meaning**:
- Low (< 1.0): High agreement among raters
- High (> 2.0): Divided opinions (context-dependent)

### 3.2 Base Score Retrieval

For each token:

```
Vâ‚€(wáµ¢) = lexicon[wáµ¢]
```

Where:
- `wáµ¢` is token i
- `Vâ‚€(wáµ¢)` is the base score retrieved from lexicon

---

## 4. Grammatical Adjustments

### 4.1 Negation

**Rule**: For each sentiment word, if a negation word appears in the **preceding 3 tokens**, invert the score

```javascript
// Negation word examples
NEGATION_WORDS = {
  "not", "no", "never", "don't", "doesn't", 
  "didn't", "can't", "won't", "couldn't",
  "isn't", "wasn't", "weren't", ...
}
```

**Application Method**:

```
for each token i with Vâ‚€(wáµ¢) â‰  0:
    for j from max(0, i-3) to i-1:
        if tokens[j] in NEGATION_WORDS:
            if j == i-3 and tokens[i-1] not in ["or", "nor"]:
                continue  // Special rule
            Vâ‚(wáµ¢) = Vâ‚€(wáµ¢) Ã— N_SCALAR
            break
```

**Parameter**:
- `N_SCALAR = -0.74`

**Why -0.74?**

Experiments in the paper found that a somewhat weaker inversion is more appropriate for social media text than simple reversal (Ã—-1.0).

**Example**:

```
"This is not very good or nice"
Position: 0   1   2    3    4    5  6

"good" (position 4):
  Preceding 3 tokens: positions 1,2,3 ("is", "not", "very")
  â†’ "not" at position 2 â†’ apply negation
  â†’ Vâ‚ = 1.9 Ã— -0.74 = -1.406

"nice" (position 6):
  Preceding 3 tokens: positions 3,4,5 ("very", "good", "or")
  â†’ No negation word â†’ no negation applied
  â†’ Vâ‚ = 1.8 (unchanged)
```

### 4.2 Boosters (Intensifiers)

**Rule**: Booster words in preceding 3 tokens increase/decrease score (with distance decay)

```javascript
BOOSTER_DICT = {
  // Positive boosters
  "very": +0.293,
  "extremely": +0.293,
  "incredibly": +0.293,
  "absolutely": +0.293,
  
  // Negative boosters (dampeners)
  "somewhat": -0.293,
  "barely": -0.293,
  "hardly": -0.293,
  "slightly": -0.293,
  ...
}
```

**Application Method**:

```
for startI from 0 to 2:
    if i > startI:
        prevToken = tokens[i - (startI + 1)]
        if prevToken not in LEXICON and prevToken in BOOSTER_DICT:
            scalar = BOOSTER_DICT[prevToken]
            if V(wáµ¢) < 0:
                scalar *= -1  // Invert for negative words
            
            // Distance decay
            if startI == 1:
                scalar *= 0.95  // Distance 2: 5% decay
            if startI == 2:
                scalar *= 0.90  // Distance 3: 10% decay
            
            Vâ‚‚(wáµ¢) = Vâ‚(wáµ¢) + scalar
```

**Theoretical Basis for Distance Decay**:

Closer boosters have stronger influence (linguistic proximity principle)

**Example**:

```
"This is very very good"
         â†‘    â†‘    â†‘
    Distance2 Distance1 Sentiment word

Distance 1 "very": scalar = +0.293 Ã— 0.95 = +0.278
Distance 2 "very": scalar = +0.293 Ã— 0.90 = +0.264
Total: Vâ‚‚ = 1.9 + 0.278 + 0.264 = 2.442
```

### 4.3 ALL CAPS Emphasis

**Rule**: When a sentiment word is in all caps and the entire text is not all caps, apply emphasis

```javascript
function allCapDifferential(tokens) {
    let allCapCount = 0;
    tokens.forEach(t => {
        if (t.text === t.text.toUpperCase() && /[A-Z]/.test(t.text)) {
            allCapCount++;
        }
    });
    return allCapCount > 0 && allCapCount < tokens.length;
}

if (word === word.toUpperCase() && isCapDiff) {
    if (V(wáµ¢) > 0) {
        Vâ‚ƒ(wáµ¢) = Vâ‚‚(wáµ¢) + C_INCR;
    } else {
        Vâ‚ƒ(wáµ¢) = Vâ‚‚(wáµ¢) - C_INCR;
    }
}
```

**Parameter**:
- `C_INCR = 0.733`

**Why only when not all text is caps?**

On social media, specific words are capitalized for emphasis. When entire text is caps (e.g., titles), there's no emphasis intent.

**Example**:

```
"This is AMAZING"
â†’ Vâ‚ƒ = 2.5 + 0.733 = 3.233

"THIS IS AMAZING"  // All caps
â†’ Vâ‚ƒ = 2.5 (unchanged)
```

### 4.4 Exclamation Marks

**Rule**: Exclamation marks immediately after a sentiment word (maximum 4) provide emphasis

```javascript
let exclamationCount = 0;
if (i + 1 < tokens.length) {
    const nextToken = tokens[i + 1];
    if (/^!+$/.test(nextToken.text)) {
        exclamationCount = Math.min(nextToken.text.length, 4);
    }
}

const boost = exclamationCount Ã— 0.292;

if (V(wáµ¢) > 0) {
    Vâ‚„(wáµ¢) = Vâ‚ƒ(wáµ¢) + boost;
} else {
    Vâ‚„(wáµ¢) = Vâ‚ƒ(wáµ¢) - boost;
}
```

**Parameters**:
- `E_INCR = 0.292` (per exclamation mark)
- Maximum 4 (no additional effect beyond 4)

**Why limit to 4?**

Paper experiments found that 5 or more exclamation marks provide no additional effect.

**Example**:

```
"This is amazing!"      â†’ boost = 1 Ã— 0.292 = +0.292
"This is amazing!!"     â†’ boost = 2 Ã— 0.292 = +0.584
"This is amazing!!!"    â†’ boost = 3 Ã— 0.292 = +0.876
"This is amazing!!!!"   â†’ boost = 4 Ã— 0.292 = +1.168
"This is amazing!!!!!"  â†’ boost = 4 Ã— 0.292 = +1.168 (ceiling)
```

### 4.5 "but" Context Adjustment

**Rule**: Sentiment before "but" is attenuated, after is emphasized

```javascript
if ("but" in tokens) {
    const butIndex = tokens.indexOf("but");
    
    for (let i = 0; i < tokens.length; i++) {
        if (V(wáµ¢) !== 0) {
            if (i < butIndex) {
                Vâ‚…(wáµ¢) = Vâ‚„(wáµ¢) Ã— 0.5;  // First half attenuated
            } else if (i > butIndex) {
                Vâ‚…(wáµ¢) = Vâ‚„(wáµ¢) Ã— 1.5;  // Second half emphasized
            }
        }
    }
}
```

**Linguistic Basis**:

"but" is a contrastive conjunction, and content following it represents the speaker's true opinion (contrastive focus)

**Example**:

```
"The book was good but the service was terrible"

"good" (before but):
  Vâ‚… = 1.9 Ã— 0.5 = 0.95

"terrible" (after but):
  Vâ‚… = -2.5 Ã— 1.5 = -3.75

â†’ Overall score is pulled negative by "terrible"
```

---

## 5. Score Normalization

### 5.1 Compound Score Calculation

Using all adjusted scores, calculate the Compound Score:

```
compound = Î£(valence_i) / âˆš(Î£(valence_iÂ²) + Î±)
```

Where Î± = 15 (normalization parameter)

**JavaScript Implementation**:

```javascript
let sum = 0;
let sumSquares = 0;

sentiments.forEach(s => {
    sum += s.adjustedScore;
    sumSquares += s.adjustedScore * s.adjustedScore;
});

const alpha = 15;
const compound = sum / Math.sqrt(sumSquares + alpha);
```

### 5.2 Why This Normalization Formula?

#### Reason 1: Normalization to -1 to +1

The square root in denominator ensures scores fall within -1 to +1 range.

#### Reason 2: Î± Smoothing

Provides stable scores even for short texts (few words).

**Example**:

```
Text 1: "good" (1 word)
  sum = 1.9
  sumSquares = 3.61
  compound = 1.9 / âˆš(3.61 + 15) = 1.9 / 4.31 = 0.441

Text 2: "good great excellent" (3 words)
  sum = 1.9 + 2.3 + 3.2 = 7.4
  sumSquares = 3.61 + 5.29 + 10.24 = 19.14
  compound = 7.4 / âˆš(19.14 + 15) = 7.4 / 5.84 = 1.267 â†’ normalized â‰ˆ 0.78
```

#### Reason 3: Dampening Extreme Values

Square root prevents extremely high/low scores from dominating excessively.

### 5.3 Positive/Negative/Neutral Calculation

Calculate proportion of each category:

```javascript
let posSum = 0, negSum = 0, neuCount = 0;

sentiments.forEach(s => {
    if (s.adjustedScore > 0) {
        posSum += s.adjustedScore;
    } else if (s.adjustedScore < 0) {
        negSum += Math.abs(s.adjustedScore);
    } else {
        neuCount++;
    }
});

const total = posSum + negSum + neuCount;
const pos = total > 0 ? posSum / total : 0;
const neg = total > 0 ? negSum / total : 0;
const neu = total > 0 ? neuCount / total : 0;
```

**Properties**:
- `pos + neg + neu â‰ˆ 1.0`
- These represent **proportions** of sentiment words (independent from Compound)

**Compound vs Pos/Neg/Neu**:

```
Example: "I love this. But I hate that."

Compound: -0.12 (overall slightly negative)
Pos: 0.45, Neg: 0.40, Neu: 0.15 (mixed positive and negative)
```

---

## 6. Implementation Details

### 6.1 Tokenization

```javascript
function tokenize(text) {
    const emojiRegex = /[\u{1F300}-\u{1F9FF}...]/u;
    const regex = /(!+|\?+|[:()\[\]{}<>*\-_|\\\/]+|[\w']+|[emoji])/gu;
    
    let tokens = [];
    let match;
    
    while ((match = regex.exec(text)) !== null) {
        tokens.push({
            text: match[0],
            index: match.index,
            lower: match[0].toLowerCase(),
            isEmoji: emojiRegex.test(match[0])
        });
    }
    
    return tokens;
}
```

**Tokenization Features**:
- Keeps exclamation/question marks as independent tokens
- Recognizes emojis individually
- Includes symbols in processing

### 6.2 ALL CAPS Detection

```javascript
function allCapDifferential(tokens) {
    let allCapCount = 0;
    
    tokens.forEach(t => {
        if (t.text === t.text.toUpperCase() && /[A-Z]/.test(t.text)) {
            allCapCount++;
        }
    });
    
    // Only true when some but not all words are caps
    return allCapCount > 0 && allCapCount < tokens.length;
}
```

### 6.3 Importance of Processing Order

Rule application in VADER has **important ordering**:

```
1. Tokenization
2. Locate "but" position
3. For each token:
   a. Get lexicon score (Vâ‚€)
   b. Check preceding 3 tokens for boosters (distance decay) â†’ Vâ‚‚
   c. Check ALL CAPS â†’ Vâ‚ƒ
   d. Check next token for exclamation marks â†’ Vâ‚„
   e. Apply "but" adjustment â†’ Vâ‚…
   f. Check preceding 3 tokens for negation â†’ V_final
4. Calculate Compound Score
```

**Why this order?**

Negation is applied last so it affects scores already adjusted by other rules.

### 6.4 Emoji Processing (Optional)

```javascript
if (token.isEmoji && emojiLexicon[token.text]) {
    const description = emojiLexicon[token.text];
    // Example: ğŸ˜Š â†’ "smiling face"
    
    // Look up each word in description using VADER lexicon
    const words = description.split(/\s+/);
    let totalScore = 0;
    
    words.forEach(word => {
        if (vaderLexicon[word]) {
            totalScore += vaderLexicon[word].score;
        }
    });
    
    // Use total score as emoji's score
}
```

---

## Appendix: Parameter List

| Parameter | Value | Purpose |
|-----------|-------|---------|
| N_SCALAR | -0.74 | Negation intensity |
| B_INCR | Â±0.293 | Booster increment/decrement |
| C_INCR | 0.733 | ALL CAPS emphasis |
| E_INCR | 0.292 | Exclamation emphasis |
| Î± | 15 | Normalization smoothing |
| Negation range | 3 tokens | Preceding check range |
| Booster range | 3 tokens | Preceding check range |
| Distance 1 decay | 0.95 | Booster distance decay |
| Distance 2 decay | 0.90 | Booster distance decay |
| Before "but" | Ã—0.5 | Score attenuation |
| After "but" | Ã—1.5 | Score emphasis |
| Exclamation max | 4 | Effect ceiling |

---

## Reference

Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. *Eighth International Conference on Weblogs and Social Media (ICWSM-14)*. Ann Arbor, MI, June 2014.

---

# æ—¥æœ¬èªç‰ˆ

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€VADERã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…è©³ç´°ã¨æ•°å­¦çš„ãªå®šç¾©ã‚’èª¬æ˜ã—ã¾ã™ã€‚

---

## ç›®æ¬¡

1. [ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¦‚è¦](#1-ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¦‚è¦-1)
2. [åˆ†æå˜ä½ï¼šæ–‡ãƒ¬ãƒ™ãƒ« vs æ–‡æ›¸ãƒ¬ãƒ™ãƒ«](#2-åˆ†æå˜ä½æ–‡ãƒ¬ãƒ™ãƒ«-vs-æ–‡æ›¸ãƒ¬ãƒ™ãƒ«)
3. [ãƒ¬ã‚­ã‚·ã‚³ãƒ³ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°](#3-ãƒ¬ã‚­ã‚·ã‚³ãƒ³ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°)
4. [æ–‡æ³•ãƒ«ãƒ¼ãƒ«ã«ã‚ˆã‚‹èª¿æ•´](#4-æ–‡æ³•ãƒ«ãƒ¼ãƒ«ã«ã‚ˆã‚‹èª¿æ•´)
5. [ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–](#5-ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–)
6. [å®Ÿè£…ã®è©³ç´°](#6-å®Ÿè£…ã®è©³ç´°)

---

## 1. ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¦‚è¦

VADERã¯ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¾ã™ï¼š

```
å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
    â†“
[0] æ–‡åˆ†å‰²ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    â†“
[1] ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ»ãƒ¬ã‚­ã‚·ã‚³ãƒ³ã‚¹ã‚³ã‚¢å–å¾—
    â†“
[2] æ–‡æ³•ãƒ«ãƒ¼ãƒ«ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢èª¿æ•´
    â†“
[3] æ­£è¦åŒ–ã¨Compound Scoreè¨ˆç®—
    â†“
å‡ºåŠ›: {compound, pos, neg, neu}
```

---

## 2. åˆ†æå˜ä½ï¼šæ–‡ãƒ¬ãƒ™ãƒ« vs æ–‡æ›¸ãƒ¬ãƒ™ãƒ«

### 2.1 VADERè«–æ–‡ã®æ¨å¥¨

VADERè«–æ–‡ï¼ˆHutto & Gilbert, 2014ï¼‰ã§ã¯**æ–‡å˜ä½ã§ã®åˆ†æã‚’æ¨å¥¨**ã—ã¦ã„ã¾ã™ï¼š

> "VADER performs **sentence-level sentiment analysis**"
> 
> "decomposing paragraphs, articles/reports/publications, or novels into **sentence-level analyses**"

### 2.2 ãªãœæ–‡å˜ä½ãªã®ã‹ï¼Ÿ

#### ç†ç”±1ï¼šæ„Ÿæƒ…ã®æ–‡è„ˆä¾å­˜æ€§

æ–‡ã«ã‚ˆã£ã¦æ„Ÿæƒ…ãŒå¤‰åŒ–ã™ã‚‹ä¾‹ï¼š

```
æ–‡æ›¸: "I love the design. But the quality is terrible."

æ–‡æ›¸å˜ä½åˆ†æ:
  â†’ Compound: +0.12 (å…¨ä½“ã¨ã—ã¦ã‚„ã‚„è‚¯å®šçš„ï¼Ÿ)

æ–‡å˜ä½åˆ†æ:
  æ–‡1: "I love the design." â†’ +0.6369 (æ˜ç¢ºã«è‚¯å®šçš„)
  æ–‡2: "But the quality is terrible." â†’ -0.5267 (æ˜ç¢ºã«å¦å®šçš„)
```

#### ç†ç”±2ï¼šè«–æ–‡ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿

VADERè«–æ–‡ã®è©•ä¾¡ã¯å…¨ã¦æ–‡å˜ä½ï¼š

| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ã‚µãƒ³ãƒ—ãƒ«æ•° | å˜ä½ |
|-------------|-----------|------|
| Movie reviews | 10,605 | sentence-level snippets |
| Product reviews | 3,708 | sentence-level snippets |
| News articles | 5,190 | sentence-level snippets |

#### ç†ç”±3ï¼šæ–‡æ³•ãƒ«ãƒ¼ãƒ«ã®é©ç”¨ç¯„å›²

VADERã®æ–‡æ³•ãƒ«ãƒ¼ãƒ«ï¼ˆå¦å®šã€"but"ãªã©ï¼‰ã¯æ–‡å†…ã§æ©Ÿèƒ½ï¼š

```
"The product is good. But I don't like it."

æ–‡å˜ä½:
  æ–‡1: "good" â†’ ãƒã‚¸ãƒ†ã‚£ãƒ–ï¼ˆ"but"ã®å½±éŸ¿ãªã—ï¼‰
  æ–‡2: "like" â†’ å¦å®šã•ã‚Œã‚‹ï¼ˆ"don't"ã®å½±éŸ¿ï¼‰

æ–‡æ›¸å˜ä½:
  "good" â†’ "but"ã®å½±éŸ¿ã§æ¸›è¡°ï¼ˆæ–‡ã‚’ã¾ãŸã„ã§é©ç”¨ï¼‰
  "like" â†’ å¦å®šã•ã‚Œã‚‹
  â†’ æ–‡æ³•ãƒ«ãƒ¼ãƒ«ãŒæ„å›³ã—ãªã„å½¢ã§é©ç”¨ã•ã‚Œã‚‹
```

### 2.3 æ–‡åˆ†å‰²ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

#### åŸºæœ¬çš„ãªåˆ†å‰²ãƒ‘ã‚¿ãƒ¼ãƒ³

```javascript
function splitIntoSentences(text) {
    // ãƒ”ãƒªã‚ªãƒ‰ã€æ„Ÿå˜†ç¬¦ã€ç–‘å•ç¬¦ã§åˆ†å‰²
    return text.split(/([.!?]+\s+)/);
}
```

#### ç•¥èªä¿è­·ã®å®Ÿè£…

ä¸€èˆ¬çš„ãªç•¥èªã§ã®èª¤åˆ†å‰²ã‚’é˜²ãï¼š

```javascript
function splitIntoSentences(text) {
    // ã‚¹ãƒ†ãƒƒãƒ—1: ç•¥èªã®ãƒ”ãƒªã‚ªãƒ‰ã‚’ä¿è­·
    const protectedText = text
        .replace(/\b(Mr|Mrs|Ms|Dr|Prof|Sr|Jr)\./gi, '$1<PERIOD>')
        .replace(/\b([A-Z])\./g, '$1<PERIOD>'); // ã‚¤ãƒ‹ã‚·ãƒ£ãƒ«
    
    // ã‚¹ãƒ†ãƒƒãƒ—2: æ–‡åˆ†å‰²
    const sentences = protectedText
        .split(/([.!?]+\s+)/)
        .reduce((acc, part, i, arr) => {
            if (i % 2 === 0) {
                const sentence = (part + (arr[i + 1] || '')).trim();
                if (sentence.length > 0) {
                    acc.push(sentence.replace(/<PERIOD>/g, '.'));
                }
            }
            return acc;
        }, []);
    
    return sentences;
}
```

**ä¿è­·ã•ã‚Œã‚‹ç•¥èªãƒªã‚¹ãƒˆï¼š**

| ã‚«ãƒ†ã‚´ãƒª | ä¾‹ |
|---------|-----|
| æ•¬ç§° | Mr., Mrs., Ms., Dr., Prof. |
| è‚©æ›¸ã | Sr., Jr. |
| ãã®ä»– | vs., etc., Inc., Ltd., Corp., Co. |
| ã‚¤ãƒ‹ã‚·ãƒ£ãƒ« | M., T., J., H. ãªã© |

#### æ–‡åˆ†å‰²ã®ä¾‹

```
å…¥åŠ›: 
"Mr. M. T. John and J. H. Samuel presented their papers."

èª¤ã£ãŸåˆ†å‰²ï¼ˆç•¥èªä¿è­·ãªã—ï¼‰:
  æ–‡1: "Mr."
  æ–‡2: "M."
  æ–‡3: "T."
  ...

æ­£ã—ã„åˆ†å‰²ï¼ˆç•¥èªä¿è­·ã‚ã‚Šï¼‰:
  æ–‡1: "Mr. M. T. John and J. H. Samuel presented their papers."
```

### 2.4 é›†è¨ˆæ–¹æ³•

æ–‡å˜ä½åˆ†æã§è¤‡æ•°ã®æ–‡ãŒã‚ã‚‹å ´åˆã®é›†è¨ˆï¼š

#### æ–¹æ³•1ï¼šå¹³å‡å€¤

```javascript
const avgCompound = sentences.reduce((sum, s) => sum + s.compound, 0) / sentences.length;
```

**é©ç”¨å ´é¢**: å…¨ä½“çš„ãªæ„Ÿæƒ…å‚¾å‘ã‚’çŸ¥ã‚ŠãŸã„å ´åˆ

#### æ–¹æ³•2ï¼šã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚«ã‚¦ãƒ³ãƒˆ

```javascript
const posCount = sentences.filter(s => s.compound >= 0.05).length;
const negCount = sentences.filter(s => s.compound <= -0.05).length;
const neuCount = sentences.length - posCount - negCount;
```

**é©ç”¨å ´é¢**: æ„Ÿæƒ…ã®æ§‹æˆã‚’åˆ†æã—ãŸã„å ´åˆ

#### æ–¹æ³•3ï¼šé‡ã¿ä»˜ãå¹³å‡

```javascript
// æ–‡ã®é•·ã•ã§é‡ã¿ä»˜ã‘
const weighted = sentences.reduce((sum, s) => {
    const weight = s.sentiments.length;
    return sum + (s.compound * weight);
}, 0) / totalTokens;
```

**é©ç”¨å ´é¢**: é•·ã„æ–‡ã‚’é‡è¦–ã—ãŸã„å ´åˆ

---

## 3. ãƒ¬ã‚­ã‚·ã‚³ãƒ³ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

### 3.1 ãƒ¬ã‚­ã‚·ã‚³ãƒ³ã®æ§‹é€ 

å„å˜èªã¯ä»¥ä¸‹ã®æƒ…å ±ã‚’æŒã¡ã¾ã™ï¼š

```
å˜èª    å¹³å‡ã‚¹ã‚³ã‚¢  æ¨™æº–åå·®   äººé–“è©•ä¾¡å€¤
good    1.9         0.94       [2,2,2,1,2,...]
bad     -1.5        0.75       [-2,-1,-2,-2,...]
```

**ã‚¹ã‚³ã‚¢ç¯„å›²**: -4 (æ¥µã‚ã¦ãƒã‚¬ãƒ†ã‚£ãƒ–) ã€œ +4 (æ¥µã‚ã¦ãƒã‚¸ãƒ†ã‚£ãƒ–)

**æ¨™æº–åå·®ã®æ„å‘³**:
- ä½ã„ï¼ˆ< 1.0ï¼‰: è©•ä¾¡è€…é–“ã§ä¸€è‡´åº¦ãŒé«˜ã„
- é«˜ã„ï¼ˆ> 2.0ï¼‰: è©•ä¾¡ãŒåˆ†ã‹ã‚Œã‚‹ï¼ˆæ–‡è„ˆä¾å­˜çš„ï¼‰

### 3.2 åŸºæœ¬ã‚¹ã‚³ã‚¢ã®å–å¾—

å„ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¤ã„ã¦ï¼š

```
Vâ‚€(wáµ¢) = lexicon[wáµ¢]
```

ã“ã“ã§ï¼š
- `wáµ¢` ã¯ãƒˆãƒ¼ã‚¯ãƒ³ i
- `Vâ‚€(wáµ¢)` ã¯ãƒ¬ã‚­ã‚·ã‚³ãƒ³ã‹ã‚‰å–å¾—ã—ãŸåŸºæœ¬ã‚¹ã‚³ã‚¢

---

## 4. æ–‡æ³•ãƒ«ãƒ¼ãƒ«ã«ã‚ˆã‚‹èª¿æ•´

### 4.1 å¦å®š (Negation)

**ãƒ«ãƒ¼ãƒ«**: å„æ„Ÿæƒ…èªã«ã¤ã„ã¦ã€ãã®**å‰æ–¹3ãƒˆãƒ¼ã‚¯ãƒ³**ã«å¦å®šèªãŒã‚ã‚Œã°ã‚¹ã‚³ã‚¢ã‚’åè»¢

```javascript
// å¦å®šèªã®ä¾‹
NEGATION_WORDS = {
  "not", "no", "never", "don't", "doesn't", 
  "didn't", "can't", "won't", "couldn't",
  "isn't", "wasn't", "weren't", ...
}
```

**é©ç”¨æ–¹æ³•**:

```
for each token i with Vâ‚€(wáµ¢) â‰  0:
    for j from max(0, i-3) to i-1:
        if tokens[j] in NEGATION_WORDS:
            if j == i-3 and tokens[i-1] not in ["or", "nor"]:
                continue  // ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«
            Vâ‚(wáµ¢) = Vâ‚€(wáµ¢) Ã— N_SCALAR
            break
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `N_SCALAR = -0.74`

**ãªãœ -0.74 ãªã®ã‹ï¼Ÿ**

è«–æ–‡ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€å˜ç´”ãªåè»¢ï¼ˆÃ—-1.0ï¼‰ã‚ˆã‚Šã‚‚ã€ã‚„ã‚„å¼±ã‚ã®åè»¢ãŒã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ†ã‚­ã‚¹ãƒˆã§ã¯é©åˆ‡ã§ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜ã€‚

**ä¾‹**:

```
"This is not very good or nice"
ä½ç½®: 0   1   2    3    4    5  6

"good" (ä½ç½®4):
  å‰æ–¹3ãƒˆãƒ¼ã‚¯ãƒ³: ä½ç½®1,2,3 ("is", "not", "very")
  â†’ "not"ãŒä½ç½®2ã«ã‚ã‚‹ â†’ å¦å®šé©ç”¨
  â†’ Vâ‚ = 1.9 Ã— -0.74 = -1.406

"nice" (ä½ç½®6):
  å‰æ–¹3ãƒˆãƒ¼ã‚¯ãƒ³: ä½ç½®3,4,5 ("very", "good", "or")
  â†’ å¦å®šèªãªã— â†’ å¦å®šé©ç”¨ãªã—
  â†’ Vâ‚ = 1.8 (å¤‰åŒ–ãªã—)
```

### 4.2 å¼·èª¿èª (Boosters)

**ãƒ«ãƒ¼ãƒ«**: å‰æ–¹3ãƒˆãƒ¼ã‚¯ãƒ³ã®å¼·èª¿èªãŒã‚¹ã‚³ã‚¢ã‚’å¢—æ¸›ï¼ˆè·é›¢ã«å¿œã˜ã¦æ¸›è¡°ï¼‰

```javascript
BOOSTER_DICT = {
  // ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ–ãƒ¼ã‚¹ã‚¿ãƒ¼
  "very": +0.293,
  "extremely": +0.293,
  "incredibly": +0.293,
  "absolutely": +0.293,
  
  // ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ–ãƒ¼ã‚¹ã‚¿ãƒ¼ï¼ˆæ¸›è¡°ï¼‰
  "somewhat": -0.293,
  "barely": -0.293,
  "hardly": -0.293,
  "slightly": -0.293,
  ...
}
```

**é©ç”¨æ–¹æ³•**:

```
for startI from 0 to 2:
    if i > startI:
        prevToken = tokens[i - (startI + 1)]
        if prevToken not in LEXICON and prevToken in BOOSTER_DICT:
            scalar = BOOSTER_DICT[prevToken]
            if V(wáµ¢) < 0:
                scalar *= -1  // ãƒã‚¬ãƒ†ã‚£ãƒ–èªã®å ´åˆã¯åè»¢
            
            // è·é›¢ã«ã‚ˆã‚‹æ¸›è¡°
            if startI == 1:
                scalar *= 0.95  // è·é›¢2: 5%æ¸›è¡°
            if startI == 2:
                scalar *= 0.90  // è·é›¢3: 10%æ¸›è¡°
            
            Vâ‚‚(wáµ¢) = Vâ‚(wáµ¢) + scalar
```

**è·é›¢æ¸›è¡°ã®ç†è«–çš„æ ¹æ‹ **:

å¼·èª¿èªãŒæ„Ÿæƒ…èªã«è¿‘ã„ã»ã©å½±éŸ¿ãŒå¼·ã„ï¼ˆè¨€èªå­¦çš„ãªè¿‘æ¥æ€§ã®åŸå‰‡ï¼‰

**ä¾‹**:

```
"This is very very good"
         â†‘    â†‘    â†‘
      è·é›¢2 è·é›¢1  æ„Ÿæƒ…èª

è·é›¢1ã®"very": scalar = +0.293 Ã— 0.95 = +0.278
è·é›¢2ã®"very": scalar = +0.293 Ã— 0.90 = +0.264
åˆè¨ˆ: Vâ‚‚ = 1.9 + 0.278 + 0.264 = 2.442
```

### 4.3 å¤§æ–‡å­—å¼·èª¿ (ALL CAPS)

**ãƒ«ãƒ¼ãƒ«**: æ„Ÿæƒ…èªãŒå¤§æ–‡å­—ã§ã€ã‹ã¤æ–‡å…¨ä½“ãŒå¤§æ–‡å­—ã§ãªã„å ´åˆã«å¼·èª¿

```javascript
function allCapDifferential(tokens) {
    let allCapCount = 0;
    tokens.forEach(t => {
        if (t.text === t.text.toUpperCase() && /[A-Z]/.test(t.text)) {
            allCapCount++;
        }
    });
    return allCapCount > 0 && allCapCount < tokens.length;
}

if (word === word.toUpperCase() && isCapDiff) {
    if (V(wáµ¢) > 0) {
        Vâ‚ƒ(wáµ¢) = Vâ‚‚(wáµ¢) + C_INCR;
    } else {
        Vâ‚ƒ(wáµ¢) = Vâ‚‚(wáµ¢) - C_INCR;
    }
}
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `C_INCR = 0.733`

**ãªãœæ–‡å…¨ä½“ãŒå¤§æ–‡å­—ã§ãªã„å ´åˆã®ã¿ã‹ï¼Ÿ**

ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ã§ã¯ã€ç‰¹å®šã®èªã‚’å¼·èª¿ã™ã‚‹ãŸã‚ã«å¤§æ–‡å­—ã‚’ä½¿ç”¨ã€‚æ–‡å…¨ä½“ãŒå¤§æ–‡å­—ã®å ´åˆï¼ˆä¾‹ï¼šã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã¯å¼·èª¿ã®æ„å›³ãŒãªã„ã€‚

**ä¾‹**:

```
"This is AMAZING"
â†’ Vâ‚ƒ = 2.5 + 0.733 = 3.233

"THIS IS AMAZING"  // å…¨ã¦å¤§æ–‡å­—
â†’ Vâ‚ƒ = 2.5 (å¤‰åŒ–ãªã—)
```

### 4.4 æ„Ÿå˜†ç¬¦ (Exclamation Marks)

**ãƒ«ãƒ¼ãƒ«**: æ„Ÿæƒ…èªã®ç›´å¾Œã®æ„Ÿå˜†ç¬¦ï¼ˆæœ€å¤§4å€‹ï¼‰ãŒå¼·èª¿

```javascript
let exclamationCount = 0;
if (i + 1 < tokens.length) {
    const nextToken = tokens[i + 1];
    if (/^!+$/.test(nextToken.text)) {
        exclamationCount = Math.min(nextToken.text.length, 4);
    }
}

const boost = exclamationCount Ã— 0.292;

if (V(wáµ¢) > 0) {
    Vâ‚„(wáµ¢) = Vâ‚ƒ(wáµ¢) + boost;
} else {
    Vâ‚„(wáµ¢) = Vâ‚ƒ(wáµ¢) - boost;
}
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `E_INCR = 0.292` (æ„Ÿå˜†ç¬¦1å€‹ã‚ãŸã‚Š)
- æœ€å¤§4å€‹ã¾ã§ï¼ˆãã‚Œä»¥ä¸Šã¯åŠ¹æœãªã—ï¼‰

**ãªãœ4å€‹ã¾ã§ï¼Ÿ**

è«–æ–‡ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€5å€‹ä»¥ä¸Šã®æ„Ÿå˜†ç¬¦ã¯è¿½åŠ åŠ¹æœãŒãªã„ã“ã¨ãŒåˆ¤æ˜ã€‚

**ä¾‹**:

```
"This is amazing!"      â†’ boost = 1 Ã— 0.292 = +0.292
"This is amazing!!"     â†’ boost = 2 Ã— 0.292 = +0.584
"This is amazing!!!"    â†’ boost = 3 Ã— 0.292 = +0.876
"This is amazing!!!!"   â†’ boost = 4 Ã— 0.292 = +1.168
"This is amazing!!!!!"  â†’ boost = 4 Ã— 0.292 = +1.168 (ä¸Šé™)
```

### 4.5 "but"ã«ã‚ˆã‚‹æ–‡è„ˆèª¿æ•´

**ãƒ«ãƒ¼ãƒ«**: "but"ã‚ˆã‚Šå‰ã®æ„Ÿæƒ…ã¯æ¸›è¡°ã€å¾Œã¯å¼·èª¿

```javascript
if ("but" in tokens) {
    const butIndex = tokens.indexOf("but");
    
    for (let i = 0; i < tokens.length; i++) {
        if (V(wáµ¢) !== 0) {
            if (i < butIndex) {
                Vâ‚…(wáµ¢) = Vâ‚„(wáµ¢) Ã— 0.5;  // å‰åŠã¯æ¸›è¡°
            } else if (i > butIndex) {
                Vâ‚…(wáµ¢) = Vâ‚„(wáµ¢) Ã— 1.5;  // å¾ŒåŠã¯å¼·èª¿
            }
        }
    }
}
```

**è¨€èªå­¦çš„æ ¹æ‹ **:

"but"ã¯å¯¾ç…§æ¥ç¶šè©ã§ã‚ã‚Šã€ãã®å¾Œã®å†…å®¹ãŒè©±è€…ã®æœ¬å½“ã®æ„è¦‹ã‚’è¡¨ã™ï¼ˆé€†æ¥ã®ç„¦ç‚¹åŒ–ï¼‰

**ä¾‹**:

```
"The book was good but the service was terrible"

"good" (but ã‚ˆã‚Šå‰):
  Vâ‚… = 1.9 Ã— 0.5 = 0.95

"terrible" (but ã‚ˆã‚Šå¾Œ):
  Vâ‚… = -2.5 Ã— 1.5 = -3.75

â†’ å…¨ä½“ã®ã‚¹ã‚³ã‚¢ã¯ "terrible" ã«å¼•ããšã‚‰ã‚Œã¦ãƒã‚¬ãƒ†ã‚£ãƒ–ã«ãªã‚‹
```

---

## 5. ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–

### 5.1 Compound Scoreã®è¨ˆç®—

ã™ã¹ã¦ã®èª¿æ•´å¾Œã‚¹ã‚³ã‚¢ã‚’ç”¨ã„ã¦ã€Compound Scoreã‚’è¨ˆç®—ã—ã¾ã™ï¼š

```
compound = Î£(valence_i) / âˆš(Î£(valence_iÂ²) + Î±)
```

ã“ã“ã§ Î± = 15ï¼ˆæ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰

**JavaScriptå®Ÿè£…**:

```javascript
let sum = 0;
let sumSquares = 0;

sentiments.forEach(s => {
    sum += s.adjustedScore;
    sumSquares += s.adjustedScore * s.adjustedScore;
});

const alpha = 15;
const compound = sum / Math.sqrt(sumSquares + alpha);
```

### 5.2 ãªãœã“ã®æ­£è¦åŒ–å¼ãªã®ã‹ï¼Ÿ

#### ç†ç”±1ï¼š-1ã€œ+1ã¸ã®æ­£è¦åŒ–

åˆ†æ¯ã®å¹³æ–¹æ ¹ã«ã‚ˆã‚Šã€ã‚¹ã‚³ã‚¢ãŒ-1ã‹ã‚‰+1ã®ç¯„å›²ã«åã¾ã‚‹ã€‚

#### ç†ç”±2ï¼šÎ±ã«ã‚ˆã‚‹ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°

çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆèªæ•°ãŒå°‘ãªã„ï¼‰ã§ã‚‚å®‰å®šã—ãŸã‚¹ã‚³ã‚¢ã‚’å‡ºã™ãŸã‚ã€‚

**ä¾‹**:

```
ãƒ†ã‚­ã‚¹ãƒˆ1: "good" (1èª)
  sum = 1.9
  sumSquares = 3.61
  compound = 1.9 / âˆš(3.61 + 15) = 1.9 / 4.31 = 0.441

ãƒ†ã‚­ã‚¹ãƒˆ2: "good great excellent" (3èª)
  sum = 1.9 + 2.3 + 3.2 = 7.4
  sumSquares = 3.61 + 5.29 + 10.24 = 19.14
  compound = 7.4 / âˆš(19.14 + 15) = 7.4 / 5.84 = 1.267 â†’ æ­£è¦åŒ–å¾Œ â‰ˆ 0.78
```

#### ç†ç”±3ï¼šæ¥µç«¯ãªå€¤ã®æŠ‘åˆ¶

å¹³æ–¹æ ¹ã«ã‚ˆã‚Šã€æ¥µç«¯ã«é«˜ã„/ä½ã„ã‚¹ã‚³ã‚¢ãŒéåº¦ã«å½±éŸ¿ã—ãªã„ã€‚

### 5.3 Positive/Negative/Neutralã®è¨ˆç®—

å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®æ¯”ç‡ã‚’è¨ˆç®—ï¼š

```javascript
let posSum = 0, negSum = 0, neuCount = 0;

sentiments.forEach(s => {
    if (s.adjustedScore > 0) {
        posSum += s.adjustedScore;
    } else if (s.adjustedScore < 0) {
        negSum += Math.abs(s.adjustedScore);
    } else {
        neuCount++;
    }
});

const total = posSum + negSum + neuCount;
const pos = total > 0 ? posSum / total : 0;
const neg = total > 0 ? negSum / total : 0;
const neu = total > 0 ? neuCount / total : 0;
```

**æ€§è³ª**:
- `pos + neg + neu â‰ˆ 1.0`
- ã“ã‚Œã‚‰ã¯æ„Ÿæƒ…èªã®**æ¯”ç‡**ã‚’è¡¨ã™ï¼ˆCompoundã¨ã¯ç‹¬ç«‹ï¼‰

**Compound vs Pos/Neg/Neu**:

```
ä¾‹: "I love this. But I hate that."

Compound: -0.12 (å…¨ä½“ã¨ã—ã¦ã‚„ã‚„å¦å®šçš„)
Pos: 0.45, Neg: 0.40, Neu: 0.15 (ãƒã‚¸ãƒ†ã‚£ãƒ–ã¨ãƒã‚¬ãƒ†ã‚£ãƒ–ãŒæ··åœ¨)
```

---

## 6. å®Ÿè£…ã®è©³ç´°

### 6.1 ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

```javascript
function tokenize(text) {
    const emojiRegex = /[\u{1F300}-\u{1F9FF}...]/u;
    const regex = /(!+|\?+|[:()\[\]{}<>*\-_|\\\/]+|[\w']+|[çµµæ–‡å­—])/gu;
    
    let tokens = [];
    let match;
    
    while ((match = regex.exec(text)) !== null) {
        tokens.push({
            text: match[0],
            index: match.index,
            lower: match[0].toLowerCase(),
            isEmoji: emojiRegex.test(match[0])
        });
    }
    
    return tokens;
}
```

**ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ç‰¹å¾´**:
- æ„Ÿå˜†ç¬¦ãƒ»ç–‘å•ç¬¦ã‚’ç‹¬ç«‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ä¿æŒ
- çµµæ–‡å­—ã‚’å€‹åˆ¥ã«èªè­˜
- è¨˜å·ã‚‚å‡¦ç†å¯¾è±¡ã«å«ã‚€

### 6.2 ALL CAPSåˆ¤å®š

```javascript
function allCapDifferential(tokens) {
    let allCapCount = 0;
    
    tokens.forEach(t => {
        if (t.text === t.text.toUpperCase() && /[A-Z]/.test(t.text)) {
            allCapCount++;
        }
    });
    
    // ä¸€éƒ¨ã®å˜èªã ã‘ãŒå¤§æ–‡å­—ã®å ´åˆã®ã¿true
    return allCapCount > 0 && allCapCount < tokens.length;
}
```

### 6.3 å‡¦ç†é †åºã®é‡è¦æ€§

VADERã®ãƒ«ãƒ¼ãƒ«é©ç”¨ã«ã¯**é †åºãŒé‡è¦**ï¼š

```
1. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
2. "but"ã®ä½ç½®ã‚’ç‰¹å®š
3. å„ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¤ã„ã¦ï¼š
   a. ãƒ¬ã‚­ã‚·ã‚³ãƒ³ã‚¹ã‚³ã‚¢å–å¾— (Vâ‚€)
   b. å‰æ–¹3ãƒˆãƒ¼ã‚¯ãƒ³ã®å¼·èª¿èªãƒã‚§ãƒƒã‚¯ï¼ˆè·é›¢æ¸›è¡°ï¼‰â†’ Vâ‚‚
   c. ALL CAPSåˆ¤å®š â†’ Vâ‚ƒ
   d. æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã®æ„Ÿå˜†ç¬¦ãƒã‚§ãƒƒã‚¯ â†’ Vâ‚„
   e. "but"ã«ã‚ˆã‚‹èª¿æ•´ â†’ Vâ‚…
   f. å‰æ–¹3ãƒˆãƒ¼ã‚¯ãƒ³ã®å¦å®šãƒã‚§ãƒƒã‚¯ â†’ V_final
4. Compound Scoreè¨ˆç®—
```

**ãªãœã“ã®é †åºï¼Ÿ**

å¦å®šã¯æœ€å¾Œã«é©ç”¨ã™ã‚‹ã“ã¨ã§ã€ä»–ã®ãƒ«ãƒ¼ãƒ«ã§èª¿æ•´ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ã«å¯¾ã—ã¦ä½œç”¨ã™ã‚‹ã€‚

### 6.4 çµµæ–‡å­—å‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

```javascript
if (token.isEmoji && emojiLexicon[token.text]) {
    const description = emojiLexicon[token.text];
    // ä¾‹: ğŸ˜Š â†’ "smiling face"
    
    // èª¬æ˜æ–‡ã®å„å˜èªã‚’VADERãƒ¬ã‚­ã‚·ã‚³ãƒ³ã§æ¤œç´¢
    const words = description.split(/\s+/);
    let totalScore = 0;
    
    words.forEach(word => {
        if (vaderLexicon[word]) {
            totalScore += vaderLexicon[word].score;
        }
    });
    
    // åˆè¨ˆã‚¹ã‚³ã‚¢ã‚’çµµæ–‡å­—ã®ã‚¹ã‚³ã‚¢ã¨ã™ã‚‹
}
```

---

## ä»˜éŒ²ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸€è¦§

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ | ç”¨é€” |
|-----------|-----|------|
| N_SCALAR | -0.74 | å¦å®šã®å¼·åº¦ |
| B_INCR | Â±0.293 | å¼·èª¿èªã®å¢—æ¸› |
| C_INCR | 0.733 | å¤§æ–‡å­—ã®å¼·èª¿ |
| E_INCR | 0.292 | æ„Ÿå˜†ç¬¦ã®å¼·èª¿ |
| Î± | 15 | æ­£è¦åŒ–ã®ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚° |
| å¦å®šç¯„å›² | 3ãƒˆãƒ¼ã‚¯ãƒ³ | å‰æ–¹ãƒã‚§ãƒƒã‚¯ç¯„å›² |
| å¼·èª¿èªç¯„å›² | 3ãƒˆãƒ¼ã‚¯ãƒ³ | å‰æ–¹ãƒã‚§ãƒƒã‚¯ç¯„å›² |
| è·é›¢1æ¸›è¡° | 0.95 | å¼·èª¿èªã®è·é›¢æ¸›è¡° |
| è·é›¢2æ¸›è¡° | 0.90 | å¼·èª¿èªã®è·é›¢æ¸›è¡° |
| butå‰æ¸›è¡° | 0.5 | butå‰ã®ã‚¹ã‚³ã‚¢èª¿æ•´ |
| butå¾Œå¼·èª¿ | 1.5 | butå¾Œã®ã‚¹ã‚³ã‚¢èª¿æ•´ |
| æ„Ÿå˜†ç¬¦ä¸Šé™ | 4å€‹ | åŠ¹æœã®ä¸Šé™ |

---

## å‚è€ƒæ–‡çŒ®

Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. *Eighth International Conference on Weblogs and Social Media (ICWSM-14)*. Ann Arbor, MI, June 2014.